const openai = require('openai');
const Anthropic = require('@anthropic-ai/sdk');
const { GoogleGenerativeAI } = require('@google/generative-ai');
const { ZhipuAI } = require('zhipuai');
const QianfanSDK = require('@baiducloud/qianfan');
const logger = require('../utils/logger');

class AIProviderService {
  constructor() {
    this.providers = new Map();

    // åˆå§‹åŒ–æ‰€æœ‰é…ç½®çš„æä¾›å•†
    this.initializeProviders();
  }

  initializeProviders() {
    // OpenAI / DeepSeek / å…¼å®¹ OpenAI API çš„æä¾›å•†
    if (process.env.OPENAI_API_KEY) {
      this.providers.set('openai', {
        name: 'OpenAI',
        type: 'openai',
        client: new openai.OpenAI({
          apiKey: process.env.OPENAI_API_KEY,
          baseURL: process.env.OPENAI_API_BASE || 'https://api.openai.com/v1'
        }),
        model: process.env.OPENAI_MODEL || 'gpt-4o',
        enabled: true
      });
    }

    if (process.env.DEEPSEEK_API_KEY) {
      this.providers.set('deepseek', {
        name: 'DeepSeek',
        type: 'openai',
        client: new openai.OpenAI({
          apiKey: process.env.DEEPSEEK_API_KEY,
          baseURL: 'https://api.deepseek.com'
        }),
        model: process.env.DEEPSEEK_MODEL || 'deepseek-chat',
        enabled: true
      });
    }

    if (process.env.GROQ_API_KEY) {
      this.providers.set('groq', {
        name: 'Groq',
        type: 'openai',
        client: new openai.OpenAI({
          apiKey: process.env.GROQ_API_KEY,
          baseURL: 'https://api.groq.com/openai/v1'
        }),
        model: process.env.GROQ_MODEL || 'llama3-70b-8192',
        enabled: true
      });
    }

    if (process.env.TOGETHER_API_KEY) {
      this.providers.set('together', {
        name: 'Together',
        type: 'openai',
        client: new openai.OpenAI({
          apiKey: process.env.TOGETHER_API_KEY,
          baseURL: 'https://api.together.xyz/v1'
        }),
        model: process.env.TOGETHER_MODEL || 'meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo',
        enabled: true
      });
    }

    if (process.env.ANTHROPIC_API_KEY) {
      this.providers.set('anthropic', {
        name: 'Anthropic (Claude)',
        type: 'anthropic',
        client: new Anthropic({
          apiKey: process.env.ANTHROPIC_API_KEY
        }),
        model: process.env.ANTHROPIC_MODEL || 'claude-3-5-sonnet-20241022',
        enabled: true
      });
    }

    if (process.env.GEMINI_API_KEY) {
      this.providers.set('gemini', {
        name: 'Google Gemini',
        type: 'gemini',
        client: new GoogleGenerativeAI(process.env.GEMINI_API_KEY),
        model: process.env.GEMINI_MODEL || 'gemini-1.5-pro',
        enabled: true
      });
    }

    if (process.env.QWEN_API_KEY) {
      this.providers.set('qwen', {
        name: 'Alibaba Qwen',
        type: 'openai',
        client: new openai.OpenAI({
          apiKey: process.env.QWEN_API_KEY,
          baseURL: process.env.QWEN_API_BASE || 'https://dashscope.aliyuncs.com/compatible-mode/v1'
        }),
        model: process.env.QWEN_MODEL || 'qwen-plus',
        enabled: true
      });
    }

    if (process.env.ZHIPU_API_KEY) {
      this.providers.set('zhipu', {
        name: 'Zhipu AI (GLM)',
        type: 'zhipu',
        client: new ZhipuAI({
          apiKey: process.env.ZHIPU_API_KEY
        }),
        model: process.env.ZHIPU_MODEL || 'glm-4',
        enabled: true
      });
    }

    if (process.env.QIANFAN_ACCESS_KEY && process.env.QIANFAN_SECRET_KEY) {
      this.providers.set('qianfan', {
        name: 'Baidu Qianfan',
        type: 'qianfan',
        client: new QianfanSDK.Qianfan(process.env.QIANFAN_ACCESS_KEY, process.env.QIANFAN_SECRET_KEY),
        model: process.env.QIANFAN_MODEL || 'ERNIE-4.0-8K',
        enabled: true
      });
    }

    if (process.env.MOONSHOT_API_KEY) {
      this.providers.set('moonshot', {
        name: 'Moonshot AI (Kimi)',
        type: 'openai',
        client: new openai.OpenAI({
          apiKey: process.env.MOONSHOT_API_KEY,
          baseURL: 'https://api.moonshot.cn/v1'
        }),
        model: process.env.MOONSHOT_MODEL || 'moonshot-v1-8k',
        enabled: true
      });
    }

    if (process.env.BAICHUAN_API_KEY) {
      this.providers.set('baichuan', {
        name: 'Baichuan AI',
        type: 'openai',
        client: new openai.OpenAI({
          apiKey: process.env.BAICHUAN_API_KEY,
          baseURL: process.env.BAICHUAN_API_BASE || 'https://api.baichuan-ai.com/v1'
        }),
        model: process.env.BAICHUAN_MODEL || 'Baichuan4',
        enabled: true
      });
    }

    if (process.env.MINIMAX_API_KEY) {
      this.providers.set('minimax', {
        name: 'MiniMax',
        type: 'openai',
        client: new openai.OpenAI({
          apiKey: process.env.MINIMAX_API_KEY,
          baseURL: process.env.MINIMAX_API_BASE || 'https://api.minimax.chat/v1'
        }),
        model: process.env.MINIMAX_MODEL || 'abab6.5s-chat',
        enabled: true
      });
    }

    if (process.env.SILICONFLOW_API_KEY) {
      this.providers.set('siliconflow', {
        name: 'SiliconFlow',
        type: 'openai',
        client: new openai.OpenAI({
          apiKey: process.env.SILICONFLOW_API_KEY,
          baseURL: 'https://api.siliconflow.cn/v1'
        }),
        model: process.env.SILICONFLOW_MODEL || 'deepseek-ai/DeepSeek-V3',
        enabled: true
      });
    }

    if (process.env.O1_API_KEY) {
      this.providers.set('o1', {
        name: 'O1',
        type: 'openai',
        client: new openai.OpenAI({
          apiKey: process.env.O1_API_KEY,
          baseURL: process.env.O1_API_BASE || 'https://api.o1.com/v1'
        }),
        model: process.env.O1_MODEL || 'o1-1',
        enabled: true
      });
    }

    // è®¾ç½®é»˜è®¤æä¾›å•†
    this.defaultProvider = process.env.AI_DEFAULT_PROVIDER || 'deepseek';
    if (!this.providers.has(this.defaultProvider)) {
      const firstEnabled = Array.from(this.providers.keys())[0];
      this.defaultProvider = firstEnabled;
      logger.warn(`é»˜è®¤æä¾›å•† ${process.env.AI_DEFAULT_PROVIDER} æœªé…ç½®ï¼Œä½¿ç”¨ ${firstEnabled}`);
    }
  }

  getProvider(providerId = null) {
    const id = providerId || this.defaultProvider;
    const provider = this.providers.get(id);

    if (!provider) {
      throw new Error(`AI æä¾›å•† ${id} æœªé…ç½®æˆ–ä¸å¯ç”¨`);
    }

    if (!provider.enabled) {
      throw new Error(`AI æä¾›å•† ${id} å·²ç¦ç”¨`);
    }

    return provider;
  }

  async chatCompletion(messages, options = {}) {
    const {
      provider = null,
      model = null,
      temperature = 0.3,
      maxTokens = 2000,
      responseFormat = null,
      stream = false
    } = options;

    const providerConfig = this.getProvider(provider);

    try {
      const startTime = Date.now();
      let result;

      switch (providerConfig.type) {
        case 'openai':
          result = await this.openaiCompletion(providerConfig, messages, {
            model: model || providerConfig.model,
            temperature,
            maxTokens,
            responseFormat,
            stream
          });
          break;

        case 'anthropic':
          result = await this.anthropicCompletion(providerConfig, messages, {
            model: model || providerConfig.model,
            temperature,
            maxTokens,
            stream
          });
          break;

        case 'gemini':
          result = await this.geminiCompletion(providerConfig, messages, {
            model: model || providerConfig.model,
            temperature,
            maxTokens
          });
          break;

        case 'zhipu':
          result = await this.zhipuCompletion(providerConfig, messages, {
            model: model || providerConfig.model,
            temperature,
            maxTokens
          });
          break;

        case 'qianfan':
          result = await this.qianfanCompletion(providerConfig, messages, {
            model: model || providerConfig.model,
            temperature,
            maxTokens
          });
          break;

        default:
          throw new Error(`ä¸æ”¯æŒçš„æä¾›å•†ç±»å‹: ${providerConfig.type}`);
      }

      const latency = Date.now() - startTime;
      logger.info(`AI è°ƒç”¨æˆåŠŸ`, {
        provider: providerConfig.name,
        model: result.model || model || providerConfig.model,
        latency,
        tokens: result.usage?.total_tokens
      });

      return result;
    } catch (error) {
      logger.error(`AI è°ƒç”¨å¤±è´¥`, {
        provider: providerConfig.name,
        error: error.message
      });
      throw error;
    }
  }

  async openaiCompletion(provider, messages, options) {
    const response = await provider.client.chat.completions.create({
      model: options.model,
      messages: messages.map(m => ({
        role: m.role,
        content: m.content
      })),
      temperature: options.temperature,
      max_tokens: options.maxTokens,
      ...(options.responseFormat && { response_format: options.responseFormat })
    });

    return {
      content: response.choices[0].message.content,
      model: response.model,
      usage: response.usage
    };
  }

  async anthropicCompletion(provider, messages, options) {
    const systemMessage = messages.find(m => m.role === 'system');
    const userMessages = messages.filter(m => m.role !== 'system');

    const response = await provider.client.messages.create({
      model: options.model,
      max_tokens: options.maxTokens,
      temperature: options.temperature,
      system: systemMessage?.content,
      messages: userMessages.map(m => ({
        role: m.role === 'assistant' ? 'assistant' : 'user',
        content: m.content
      }))
    });

    return {
      content: response.content[0].text,
      model: response.model,
      usage: {
        prompt_tokens: response.usage.input_tokens,
        completion_tokens: response.usage.output_tokens,
        total_tokens: response.usage.input_tokens + response.usage.output_tokens
      }
    };
  }

  async geminiCompletion(provider, messages, options) {
    const model = provider.client.getGenerativeModel({ model: options.model });

    const systemMessage = messages.find(m => m.role === 'system');
    const userMessages = messages.filter(m => m.role !== 'system');

    const prompt = userMessages.map(m => `${m.role}: ${m.content}`).join('\n');

    const generationConfig = {
      temperature: options.temperature,
      maxOutputTokens: options.maxTokens,
      responseMimeType: 'text/plain'
    };

    const result = await model.generateContent({
      contents: [{ role: 'user', parts: [{ text: prompt }] }],
      generationConfig,
      ...(systemMessage && { systemInstruction: systemMessage.content })
    });

    const response = result.response;
    return {
      content: response.text(),
      model: options.model,
      usage: {
        prompt_tokens: response.usageMetadata?.promptTokenCount || 0,
        completion_tokens: response.usageMetadata?.candidatesTokenCount || 0,
        total_tokens: response.usageMetadata?.totalTokenCount || 0
      }
    };
  }

  async zhipuCompletion(provider, messages, options) {
    const response = await provider.client.chat.completions.create({
      model: options.model,
      messages: messages.map(m => ({
        role: m.role === 'system' ? 'system' : 'user',
        content: m.content
      })),
      temperature: options.temperature,
      max_tokens: options.maxTokens
    });

    return {
      content: response.choices[0].message.content,
      model: options.model,
      usage: {
        prompt_tokens: response.usage?.prompt_tokens || 0,
        completion_tokens: response.usage?.completion_tokens || 0,
        total_tokens: response.usage?.total_tokens || 0
      }
    };
  }

  async qianfanCompletion(provider, messages, options) {
    const systemMessage = messages.find(m => m.role === 'system');
    const userMessages = messages.filter(m => m.role !== 'system');

    const response = await provider.client.chat.completions.create({
      model: options.model,
      messages: userMessages.map(m => ({
        role: m.role === 'assistant' ? 'assistant' : 'user',
        content: m.content
      })),
      system: systemMessage?.content,
      temperature: options.temperature,
      max_output_tokens: options.maxTokens
    });

    return {
      content: response.body.result || response.body.choices?.[0]?.message?.content,
      model: options.model,
      usage: {
        prompt_tokens: response.body.usage?.prompt_tokens || 0,
        completion_tokens: response.body.usage?.completion_tokens || 0,
        total_tokens: response.body.usage?.total_tokens || 0
      }
    };
  }

  async analyzeTopics(topics, options = {}) {
    const {
      provider = null,
      includeTrends = true,
      includeSentiment = true,
      includeKeywords = true,
      includeSummary = false
    } = options;

    if (!topics || topics.length === 0) {
      return null;
    }

    const topicsData = topics.map(t => ({
      title: t.title,
      source: t.source,
      heat: t.heat,
      description: t.description || '',
      keywords: t.keywords || [],
      category: t.category,
      publishedAt: t.publishedAt
    }));

    const prompt = this.buildAnalysisPrompt(topicsData, options);

    try {
      const result = await this.chatCompletion([
        {
          role: 'system',
          content: `ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çƒ­ç‚¹è¯é¢˜åˆ†æåŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„çƒ­ç‚¹è¯é¢˜æ•°æ®ï¼Œç”Ÿæˆç»“æ„åŒ–çš„åˆ†ææŠ¥å‘Šã€‚è¾“å‡ºæ ¼å¼å¿…é¡»æ˜¯çº¯ JSONï¼Œä¸è¦åŒ…å«ä»»ä½• Markdown æ ¼å¼æˆ–é¢å¤–è¯´æ˜ã€‚`
        },
        {
          role: 'user',
          content: prompt
        }
      ], {
        provider,
        temperature: 0.3,
        maxTokens: 2000,
        responseFormat: { type: 'json_object' }
      });

      const analysis = JSON.parse(result.content);

      logger.info('AI åˆ†æå®Œæˆ');
      return analysis;
    } catch (error) {
      logger.error('AI åˆ†æå¤±è´¥', {
        error: error.message
      });
      return null;
    }
  }

  buildAnalysisPrompt(topicsData, options) {
    const topicsList = JSON.stringify(topicsData, null, 2);

    let prompt = `è¯·åˆ†æä»¥ä¸‹ ${topicsData.length} æ¡çƒ­ç‚¹è¯é¢˜æ•°æ®ï¼š

${topicsList}

`;

    if (options.includeTrends) {
      prompt += `

è¯·æä¾›è¶‹åŠ¿åˆ†æï¼š
1. æ€»ä½“è¶‹åŠ¿æ¦‚è¿°ï¼ˆ50å­—å†…ï¼‰
2. ä¸Šå‡è¶‹åŠ¿è¯é¢˜ï¼ˆç”¨ ğŸ”º æ ‡è®°ï¼‰
3. ä¸‹é™è¶‹åŠ¿è¯é¢˜ï¼ˆç”¨ ğŸ”» æ ‡è®°ï¼‰
4. çˆ†å‘çƒ­ç‚¹ï¼ˆç”¨ ğŸ”¥ æ ‡è®°ï¼‰
`;
    }

    if (options.includeSentiment) {
      prompt += `

è¯·è¿›è¡Œæƒ…æ„Ÿåˆ†æï¼š
1. æ­£é¢è¯é¢˜ï¼ˆç”¨ ğŸ˜Š æ ‡è®°ï¼‰
2. è´Ÿé¢è¯é¢˜ï¼ˆç”¨ ğŸ˜” æ ‡è®°ï¼‰
3. äº‰è®®è¯é¢˜ï¼ˆç”¨ âš  æ ‡è®°ï¼‰
4. ä¸­æ€§è¯é¢˜ï¼ˆç”¨ ğŸ˜ æ ‡è®°ï¼‰
`;
    }

    if (options.includeKeywords) {
      prompt += `

è¯·æå–é«˜é¢‘å…³é”®è¯ï¼š
1. ç»Ÿè®¡å‡ºç°é¢‘ç‡æœ€é«˜çš„ 5-8 ä¸ªå…³é”®è¯
2. æŒ‰ç…§å¹³å°åˆ†ç»„ç»Ÿè®¡
`;
    }

    if (options.includeSummary) {
      prompt += `

è¯·ç”Ÿæˆç®€æŠ¥ï¼š
1. 3 æ¡æœ€é‡è¦çš„çƒ­ç‚¹åŠç®€è¦è¯´æ˜
2. é€‚åˆæ¨é€çš„ç²¾ç®€å†…å®¹`;
    }

    prompt += `

è¯·ä»¥ä»¥ä¸‹ JSON æ ¼å¼è¿”å›ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡å­—ï¼š
{
  "trendOverview": "è¶‹åŠ¿æ¦‚è¿°",
  "risingTopics": ["è¯é¢˜1", "è¯é¢˜2"],
  "fallingTopics": ["è¯é¢˜1", "è¯é¢˜2"],
  "hotTopics": ["è¯é¢˜1", "è¯é¢˜2"],
  "sentiment": {
    "positive": ["è¯é¢˜1"],
    "negative": ["è¯é¢˜1"],
    "controversial": ["è¯é¢˜1"],
    "neutral": ["è¯é¢˜1"]
  },
  "topKeywords": [
    {"keyword": "å…³é”®è¯", "count": æ¬¡æ•°, "sources": ["å¹³å°1", "å¹³å°2"]}
  ],
  "briefing": ["ç®€æŠ¥1", "ç®€æŠ¥2", "ç®€æŠ¥3"]
}`;

    return prompt;
  }

  async translateMessage(message, targetLanguage = 'English', provider = null) {
    try {
      const result = await this.chatCompletion([
        {
          role: 'system',
          content: `ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘åŠ©æ‰‹ã€‚è¯·å°†ä»¥ä¸‹å†…å®¹ç¿»è¯‘ä¸º ${targetLanguage}ã€‚`
        },
        {
          role: 'user',
          content: message
        }
      ], {
        provider,
        temperature: 0.3,
        maxTokens: 1000
      });

      logger.info('AI ç¿»è¯‘å®Œæˆ');
      return result.content;
    } catch (error) {
      logger.error('AI ç¿»è¯‘å¤±è´¥', { error: error.message });
      return message;
    }
  }

  async generateBrief(topics, options = {}) {
    const {
      maxLength = 300,
      focus = 'important',
      provider = null
    } = options;

    const topicsData = topics.slice(0, 10).map(t => ({
      title: t.title,
      source: t.source,
      heat: t.heat,
      description: t.description || ''
    }));

    const prompt = `è¯·ä¸ºä»¥ä¸‹çƒ­ç‚¹è¯é¢˜ç”Ÿæˆç²¾ç®€æ¨é€å†…å®¹ï¼ˆ${maxLength} å­—ä»¥å†…ï¼‰ï¼Œé‡ç‚¹ï¼š${focus}ï¼š

${JSON.stringify(topicsData, null, 2)}

è¦æ±‚ï¼š
1. æå–æœ€é‡è¦çš„ 3-5 ä¸ªè¯é¢˜
2. æ¯ä¸ªè¯é¢˜ä¸€å¥è¯æè¿°
3. ä½¿ç”¨ç®€æ´æœ‰åŠ›çš„è¯­è¨€
4. ä¸è¦ä½¿ç”¨ Markdown æ ¼å¼`;

    try {
      const result = await this.chatCompletion([
        {
          role: 'system',
          content: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–è¾‘åŠ©æ‰‹ã€‚'
        },
        {
          role: 'user',
          content: prompt
        }
      ], {
        provider,
        temperature: 0.5,
        maxTokens: 500
      });

      logger.info('AI ç®€æŠ¥ç”Ÿæˆå®Œæˆ');
      return result.content;
    } catch (error) {
      logger.error('AI ç®€æŠ¥ç”Ÿæˆå¤±è´¥', { error: error.message });
      return null;
    }
  }

  async checkServiceHealth(providerId = null) {
    const providersToCheck = providerId
      ? [providerId]
      : Array.from(this.providers.keys());

    const results = {};

    for (const id of providersToCheck) {
      try {
        const startTime = Date.now();

        await this.chatCompletion([
          { role: 'user', content: 'ping' }
        ], {
          provider: id,
          maxTokens: 10
        });

        results[id] = {
          healthy: true,
          latency: Date.now() - startTime
        };
      } catch (error) {
        results[id] = {
          healthy: false,
          error: error.message
        };
      }
    }

    return {
      defaultProvider: this.defaultProvider,
      providers: results
    };
  }

  getProviderList() {
    const list = [];

    for (const [id, config] of this.providers.entries()) {
      list.push({
        id,
        name: config.name,
        type: config.type,
        model: config.model,
        enabled: config.enabled,
        isDefault: id === this.defaultProvider
      });
    }

    return list;
  }

  setDefaultProvider(providerId) {
    if (!this.providers.has(providerId)) {
      throw new Error(`æä¾›å•† ${providerId} ä¸å­˜åœ¨`);
    }

    this.defaultProvider = providerId;
    logger.info(`é»˜è®¤æä¾›å•†å·²è®¾ç½®ä¸º ${providerId}`);
  }
}

module.exports = new AIProviderService();
